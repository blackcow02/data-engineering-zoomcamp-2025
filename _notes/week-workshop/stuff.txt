import subprocess
import sys

# Install dlt with DuckDB support
subprocess.check_call([sys.executable, "-m", "pip", "install", "dlt[duckdb]"])



import dlt
print("dlt version:", dlt.__version__)


import dlt
from dlt.sources.helpers.rest_client import RESTClient
from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator


# your code is here



# Define the API resource for NYC taxi data
# STEP 01: Use the @dlt.resource decorator to define the API source.
@dlt.resource(name="rides")   # <--- The name of the resource (will be used as the table name)

# STEP 02: Implement automatic pagination using dlt's built-in REST client.

def ny_taxi():
    client = RESTClient(
        base_url="https://us-central1-dlthub-analytics.cloudfunctions.net",
        paginator=PageNumberPaginator(
            base_page=1,
            total_path=None
        )
    )

    for page in client.paginate("data_engineering_zoomcamp_api"):    # <--- API endpoint for retrieving taxi ride data
        yield page   # <--- yield data to manage memory


# define new dlt pipeline
pipeline = dlt.pipeline(
    pipeline_name="ny_taxi_pipeline",
    destination="duckdb",
    dataset_name="ny_taxi_data"
)

# STEP 03: Load the extracted data into DuckDB for querying.
# load the data into DuckDB to test
#load_info = pipeline.run(ny_taxi, write_disposition="replace")
#print(load_info)

# explore loaded data
#pipeline.dataset(dataset_type="default").rides.df()


#

# Run the pipeline and load data into DuckDB
load_info = pipeline.run(ny_taxi(), write_disposition="replace")  # Call ny_taxi()
print("Pipeline completed successfully!")
print(load_info)

# Access and print DataFrame of 'rides' dataset
try:
    # Get the DataFrame of 'rides' from the default dataset
    df = pipeline.dataset(dataset_type="default").rides.df()
    print(df.head())  # Show the first few rows of the DataFrame

# Count the total number of rows
    total_rows = len(df)  # or df.shape[0]
    print(f"Total rows in the 'rides' dataset: {total_rows}")


except Exception as e:
    print("Error retrieving dataset:", e)



# Start a connection to your database using native duckdb connection and look what tables were generated:

import duckdb
# the lines below are used in Google Caolab to display data in a table format
#from google.colab import data_table
#data_table.enable_dataframe_formatter()

# here we will use pandas instead
import pandas as pd

# A database '<pipeline_name>.duckdb' was created in working directory so just connect to it

# Connect to the DuckDB database
conn = duckdb.connect(f"{pipeline.pipeline_name}.duckdb")

# Set search path to the dataset
conn.sql(f"SET search_path = '{pipeline.dataset_name}'")

# Describe the dataset and load it into a Pandas DataFrame
df = conn.sql("DESCRIBE").df()

# Display the DataFrame
print(df)
total_rows = len(df)  # or df.shape[0]
print(f"Total tables in DuckDB: {total_rows}")


df = pipeline.dataset(dataset_type="default").rides.df()
print(df.info())




with pipeline.sql_client() as client:
    res = client.execute_sql(
            """
            SELECT
            AVG(date_diff('minute', trip_pickup_date_time, trip_dropoff_date_time))
            FROM rides;
            """
        )
    # Prints column values of the first row
    print(res)


